# Largely arbitrarily chosen. We want to trade off density and speed.
BON_JAILBREAKING_AUDIO_Ks_LIST = [
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    12,
    14,
    16,
    18,
    20,
    24,
    28,
    32,
    36,
    40,
    44,
    48,
    52,
    56,
    64,
    68,
    72,
    76,
    80,
    84,
    88,
    92,
    96,
    100,
    105,
    110,
    115,
    120,
    125,
    130,
    135,
    140,
    145,
    150,
    156,
    162,
    168,
    175,
    182,
    189,
    193,
    200,
    216,
    233,
    267,
    300,
    333,
    367,
    400,
    433,
    467,
    500,
    550,
    600,
    650,
    700,
    750,
    800,
    850,
    900,
    950,
    1000,
    1067,
    1133,
    1200,
    1367,
    1500,
    1750,
    2000,
    2333,
    2667,
    3000,
    3333,
    3667,
    4000,
    4333,
    4667,
    5000,
    5333,
    5667,
    6000,
    6333,
    6667,
    7000,
]

BON_JAILBREAKING_AUDIO_MODELS_ORDER = [
    "DiVA",
    "Gemini 1.5 Flash",
    "Gemini 1.5 Pro",
    "GPT4o (S2S)",
]

BON_JAILBREAKING_GROUPBY_COLS = ["Model", "Modality", "Temperature"]

BON_JAILBREAKING_TEXT_Ks_LIST = [
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    12,
    14,
    16,
    18,
    20,
    24,
    28,
    32,
    36,
    40,
    44,
    48,
    52,
    56,
    64,
    68,
    72,
    76,
    80,
    84,
    88,
    92,
    96,
    100,
    105,
    110,
    115,
    120,
    125,
    130,
    135,
    140,
    145,
    150,
    156,
    162,
    168,
    175,
    182,
    189,
    193,
    200,
    216,
    233,
    267,
    300,
    333,
    367,
    400,
    433,
    467,
    500,
    550,
    600,
    650,
    700,
    750,
    800,
    850,
    900,
    950,
    1000,
    1067,
    1133,
    1200,
    1367,
    1500,
    1750,
    2000,
    2333,
    2667,
    3000,
    3333,
    3667,
    4000,
    4333,
    4667,
    5000,
    5333,
    5667,
    6000,
    6333,
    6667,
    7000,
    7333,
    7667,
    8000,
    8500,
    9000,
    9500,
    10000,
]

BON_JAILBREAKING_MODALITY_TO_NICE_STRINGS = {
    "audio": "Audio",
    "text": "Text",
    "vision": "Vision",
}

BON_JAILBREAKING_TEXT_MODELS_ORDER = [
    "Claude 3.5 Sonnet",
    "Claude 3.5 Opus",
    "Gemini 1.5 Flash",
    "Gemini 1.5 Pro",
    "GPT4o Mini",
    "GPT4o",
    "Llama 3 8B IT",
]

BON_JAILBREAKING_VISION_Ks_LIST = [
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    12,
    14,
    16,
    18,
    20,
    24,
    28,
    32,
    36,
    40,
    44,
    48,
    52,
    56,
    64,
    68,
    72,
    76,
    80,
    84,
    88,
    92,
    96,
    100,
    105,
    110,
    115,
    120,
    125,
    130,
    135,
    140,
    145,
    150,
    156,
    162,
    168,
    175,
    182,
    189,
    193,
    200,
    216,
    233,
    267,
    300,
    333,
    367,
    400,
    433,
    467,
    500,
    550,
    600,
    650,
    700,
    750,
    800,
    850,
    900,
    950,
    1000,
    1067,
    1133,
    1200,
    1367,
    1500,
    1750,
    2000,
    2333,
    2667,
    3000,
    3333,
    3667,
    4000,
    4333,
    4667,
    5000,
    5333,
    5667,
    6000,
    6333,
    6667,
    7000,
]

BON_JAILBREAKING_VISION_MODELS_ORDER = [
    "Claude 3.5 Sonnet",
    "Claude 3.5 Opus",
    "Gemini 1.5 Flash",
    "Gemini 1.5 Pro",
    "GPT4o Mini",
    "GPT4o",
]

BON_JAILBREAKING_MODELS_TO_NICE_STRINGS = {
    "claude-3-5-sonnet-20240620": "Claude 3.5 Sonnet",
    "claude-3-opus-20240229": "Claude 3.5 Opus",
    "DiVA": "DiVA",
    "gemini-1.5-flash-001": "Gemini 1.5 Flash",
    "gemini-1.5-pro-001": "Gemini 1.5 Pro",
    "gpt-4o-mini": "GPT4o Mini",
    "gpt-4o": "GPT4o",
    "gpt-4o-s2s": "GPT4o (S2S)",
    "meta-llama-Meta-Llama-3-8B-Instruct": "Llama 3 8B IT",
}

CAUSAL_LANGUAGE_MODELING_DATASETS_ORDER = [
    "The Pile",
    "MiniPile",
    "C4",
    "RedPajama",
    "LAMBADA",
    "Fineweb",
    "Zyda-2",
]

EVALUATE_SAMPLING_STRATEGY_DEFAULT_CONFIG = {
    "dataset_name": "large_language_monkeys_pythia_math",
    "dataset_kwargs": {
        "Benchmark": "MATH",
        "Model": "Pythia 70M",
        # "Modality": "text",
        # "Temperature": 1.0,
    },
    "num_problems_to_sample_from": 32,
    # "num_problems_to_sample_from": 64,
    # "num_problems_to_sample_from": 128,
    "sampling_strategy": "uniform",
    "sampling_strategy_kwargs": {},
    "seed": 0,
    "total_samples_budget": 128,
    # "total_samples_budget": 1024,
    # "total_samples_budget": 4096,
    # "total_samples_budget": 64000,
    # "total_samples_budget": 128000,
}

FIT_AND_SCORE_ESTIMATORS_DEFAULT_CONFIG = {
    # "dataset_name": "bon_jailbreaking_text",
    # "dataset_kwargs": {
    #     "Model": "GPT4o",
    #     "Modality": "Text",
    #     "Temperature": 1.0,
    # },
    # "dataset_name": "large_language_monkeys_pythia_math",
    # "dataset_kwargs": {
    #     "Benchmark": "MATH",
    #     "Model": "Pythia 70M",
    #     # "Modality": "text",
    #     # "Temperature": 1.0,
    # },
    "dataset_name": "synthetic",
    "dataset_kwargs": {
        "distribution": "beta",
        "a": 0.1,
        "b": 5.0,
        "loc": 0.0,
        "scale": 1.0,
    },
    "num_problems": 64,
    "num_samples_per_problem": 1000,
    # "seed": 0,
    "seed": 3,
}

LARGE_LANGUAGE_MONKEYS_BENCHMARKS_TO_NICE_STRINGS = {"gsm8k": "GSM8K", "math": "MATH"}

LARGE_LANGUAGE_MONKEYS_CODING_MODELS_ORDER = [
    "Llama 3 8B",
    "Llama 3 8B Instruct",
    "Llama 3 70B Instruct",
    "Gemma 2B",
    "Gemma 7B",
]

LARGE_LANGUAGE_MONKEYS_GROUPBY_COLS = ["Model", "Benchmark"]

LARGE_LANGUAGE_MONKEYS_MINI_F2F_MODELS_ORDER = [
    "Llama 3 8B Instruct",
    "Llama 3 70B Instruct",
]

LARGE_LANGUAGE_MONKEYS_ORIGINAL_Ks_LIST = [
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    12,
    14,
    16,
    18,
    20,
    24,
    28,
    32,
    36,
    40,
    44,
    48,
    52,
    56,
    64,
    68,
    72,
    76,
    80,
    84,
    88,
    92,
    96,
    100,
    105,
    110,
    115,
    120,
    125,
    130,
    135,
    140,
    145,
    150,
    156,
    162,
    168,
    175,
    182,
    189,
    193,
    200,
    216,
    233,
    267,
    300,
    333,
    367,
    400,
    433,
    467,
    500,
    550,
    600,
    650,
    700,
    750,
    800,
    850,
    900,
    950,
    1000,
    1067,
    1133,
    1200,
    1367,
    1500,
    1750,
    2000,
    2333,
    2667,
    3000,
    3333,
    3667,
    4000,
    4333,
    4667,
    5000,
    5333,
    5667,
    6000,
    6333,
    6667,
    7000,
    7333,
    7667,
    8000,
    8500,
    9000,
    9500,
    10000,
]

LARGE_LANGUAGE_MONKEYS_PYTHIA_MODELS_ORDER = [
    "Pythia 70M",
    "Pythia 160M",
    "Pythia 410M",
    "Pythia 1B",
    "Pythia 2.8B",
    "Pythia 6.9B",
    "Pythia 12B",
]

LARGE_LANGUAGE_MONKEYS_PYTHIA_MODELS_TO_NICE_STRINGS = {
    "Pythia_70M_300B": "Pythia 70M",
    "Pythia_160M_300B": "Pythia 160M",
    "Pythia_410M_300B": "Pythia 410M",
    "Pythia_1B_300B": "Pythia 1B",
    "Pythia_2.8B_300B": "Pythia 2.8B",
    "Pythia_6.9B_300B": "Pythia 6.9B",
    "Pythia_12B_300B": "Pythia 12B",
}

MANY_SHOT_IN_CONTEXT_LEARNING_DATASET_ORDER = [
    "CommonsenseQA",
    "CREAK",
    "LogiQA",
    "TriviaQA",
    "TruthfulQA",
]

PRETRAINING_MATH_DATASETS_ORDER = [
    "MATH",
    "GSM8K",
]

PRETRAINING_MATH_MODEL_FAMILY_ORDER = [
    "Pythia",
]
